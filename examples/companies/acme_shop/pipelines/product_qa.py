#!/usr/bin/env python3
"""
Product Q&A Pipeline for Acme Shop.

This pipeline demonstrates RAG-based question answering using product documentation.
It chunks markdown docs, embeds them into Qdrant, performs semantic search, and generates answers.

Usage:
    python pipelines/product_qa.py "Can I machine-wash this hoodie?"
    python pipelines/product_qa.py "Is this tent waterproof?"
"""

import json
import sys
from pathlib import Path
from typing import Any

# Add parent directories to path
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))

try:
    from sibyl.core.artifacts import ChunkArtifact
    from sibyl.workspace import Workspace
except ImportError:
    # Fallback for standalone demo
    from dataclasses import dataclass, field

    @dataclass
    class ChunkArtifact:
        content: str
        metadata: dict[str, Any] = field(default_factory=dict)

    class Workspace:
        pass


def load_and_chunk_docs(workspace: Workspace) -> list[ChunkArtifact]:
    """Load markdown documents and chunk them."""

    # Get document source provider
    doc_source = workspace.providers.get("document_sources", {}).get("product_docs")
    if not doc_source:
        msg = "Product docs source not configured"
        raise ValueError(msg)

    # Load documents from filesystem
    docs_path = Path(__file__).parent.parent / "data" / "docs"
    markdown_files = list(docs_path.glob("*.md"))

    # Simple chunking (in real implementation, would use chunking technique)
    chunks = []
    for md_file in markdown_files:
        content = md_file.read_text()

        # Split on headers for semantic chunks
        sections = content.split("\n## ")
        for i, section in enumerate(sections):
            if not section.strip():
                continue

            # Reconstruct header
            if i > 0:
                section = "## " + section

            # Create chunk artifact
            chunk = ChunkArtifact(
                content=section.strip(),
                metadata={"source": md_file.name, "chunk_index": i, "type": "markdown_section"},
            )
            chunks.append(chunk)

    return chunks


def embed_and_index(chunks: list[ChunkArtifact], workspace: Workspace) -> None:
    """Embed chunks and store in Qdrant."""

    # In real implementation, would use embedding provider and vector store
    # For this demo, we'll simulate the process


def retrieve_relevant_chunks(
    question: str, workspace: Workspace, top_k: int = 5
) -> list[ChunkArtifact]:
    """Retrieve relevant chunks for the question."""

    # Simulated retrieval - in real implementation would use vector search
    # For demo, return mock relevant chunks based on keywords

    docs_path = Path(__file__).parent.parent / "data" / "docs"
    relevant_chunks = []

    # Simple keyword matching for demo
    keywords = question.lower().split()

    for md_file in docs_path.glob("*.md"):
        content = md_file.read_text()

        # Check if keywords appear in document
        content_lower = content.lower()
        relevance = sum(1 for kw in keywords if kw in content_lower)

        if relevance > 0:
            # Extract most relevant section
            sections = content.split("\n## ")
            for section in sections[:3]:  # Top 3 sections
                if any(kw in section.lower() for kw in keywords):
                    chunk = ChunkArtifact(
                        content=section.strip()[:500],  # Truncate for demo
                        metadata={
                            "source": md_file.name,
                            "relevance_score": relevance / len(keywords),
                        },
                    )
                    relevant_chunks.append(chunk)
                    break

    return relevant_chunks[:top_k]


def generate_answer(
    question: str, context_chunks: list[ChunkArtifact], workspace: Workspace
) -> str:
    """Generate answer using LLM with retrieved context."""

    # Build context from chunks
    context = "\n\n---\n\n".join(
        [
            f"From {chunk.metadata.get('source', 'unknown')}:\n{chunk.content}"
            for chunk in context_chunks
        ]
    )

    # In real implementation, would call LLM provider
    # For demo, create a simulated answer
    return f"""Based on the product documentation:

{context[:500]}...

[This is a simulated answer. In production, this would be generated by GPT-4 based on the full context.]

Key points extracted from the documentation:
- The relevant product information was found in {len(context_chunks)} documents
- Each document provides specific care instructions and product details
- For the most accurate information, please refer to the product page

Would you like more specific information about any particular product?
"""


def run_product_qa_pipeline(question: str) -> dict[str, Any]:
    """Execute the product Q&A pipeline."""

    # Load workspace (in real implementation)
    # workspace = Workspace.from_yaml("config/workspace.yaml")
    workspace = None  # Placeholder for demo

    # Step 1: Load and chunk documents (if not already done)
    # chunks = load_and_chunk_docs(workspace)

    # Step 2: Embed and index (if not already done)
    # embed_and_index(chunks, workspace)

    # Step 3: Retrieve relevant chunks
    relevant_chunks = retrieve_relevant_chunks(question, workspace)

    # Step 4: Generate answer
    answer = generate_answer(question, relevant_chunks, workspace)

    # Prepare result
    result = {
        "question": question,
        "answer": answer,
        "sources": [chunk.metadata.get("source") for chunk in relevant_chunks],
        "num_chunks_retrieved": len(relevant_chunks),
    }

    for _source in set(result["sources"]):
        pass

    return result


def main() -> None:
    """Main entry point."""
    if len(sys.argv) < 2:  # noqa: PLR2004
        sys.exit(1)

    question = " ".join(sys.argv[1:])

    try:
        result = run_product_qa_pipeline(question)

        # Save result
        output_file = Path(__file__).parent.parent / "output" / "product_qa_result.json"
        output_file.parent.mkdir(exist_ok=True)

        with open(output_file, "w") as f:
            json.dump(result, f, indent=2)

    except Exception:
        import traceback  # noqa: PLC0415 - can be moved to top

        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
