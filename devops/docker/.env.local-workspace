# ============================================================================
# Sibyl Local Development Workspace Configuration
# ============================================================================
# Use this configuration for local development with Ollama LLM and
# local embeddings using Sentence Transformers.
#
# Usage:
#   docker-compose --env-file .env.local-workspace --profile multi-workspace up
#
# ============================================================================

# Workspace
SIBYL_WORKSPACE_LOCAL=config/workspaces/example_local.yaml
SIBYL_WORKSPACE_PROD=config/workspaces/example_local.yaml  # Use local for both

# MCP Servers
MCP_HTTP_HOST=0.0.0.0
MCP_HTTP_PORT=8770
MCP_LOCAL_WORKSPACE_PORT=8771

# HTTP Servers
HTTP_LOCAL_WORKSPACE_PORT=8000

# Logging
MCP_LOG_LEVEL=DEBUG
MCP_JSON_LOGS=false

# Resources (Low for local dev)
DUCKDB_MEMORY_LIMIT=1GB

# Build
BUILD_TARGET=development
VERSION=dev

# ============================================================================
# Notes:
# - Local workspace uses Ollama (running on localhost:11434)
# - If Ollama not running: docker run -d -v ollama:/root/.ollama -p 11434:11434 ollama/ollama
# - Pull model: docker exec ollama ollama pull llama2
# ============================================================================
