# Fast training configuration for 1.5B parameter model
# Optimized for Apple Silicon M1 with 32GB RAM
# Expected training time: ~30-60 minutes on M1

model:
  # Base model from HuggingFace
  base_model: "Qwen/Qwen2.5-1.5B-Instruct"

  # LoRA (Low-Rank Adaptation) parameters
  lora_rank: 16
  lora_alpha: 32
  lora_dropout: 0.05

  # Modules to apply LoRA to
  target_modules:
    - "q_proj"
    - "k_proj"
    - "v_proj"
    - "o_proj"
    - "gate_proj"
    - "up_proj"
    - "down_proj"

  # Quantization for memory efficiency
  load_in_4bit: true
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"

training:
  # Training hyperparameters
  num_epochs: 3
  batch_size: 4
  gradient_accumulation_steps: 4  # Effective batch size = 16

  # Learning rate
  learning_rate: 2.0e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03

  # Optimization
  optimizer: "adamw_8bit"
  weight_decay: 0.01
  max_grad_norm: 1.0

  # Memory optimization
  gradient_checkpointing: true
  max_seq_length: 2048
  packing: false  # Set to true to pack multiple examples into one sequence

  # Logging and saving
  logging_steps: 10
  save_steps: 100
  save_total_limit: 3
  eval_steps: 50
  evaluation_strategy: "steps"

  # Mixed precision
  fp16: false
  bf16: true  # Better for M1

  # Other settings
  seed: 42
  output_dir: "outputs/toy_fast_1_5b"

data:
  # Data paths (relative to training_toolkit/)
  train_path: "data/toy_assistant/train.jsonl"
  dev_path: "data/toy_assistant/dev.jsonl"
  test_path: "data/toy_assistant/test.jsonl"

  # Data processing
  prompt_template: |
    ### Instruction:
    {prompt}

    ### Response:
    {completion}

  # Dataset parameters
  dataset_text_field: "text"
  max_prompt_length: 512
  max_completion_length: 1536

hardware:
  # Hardware-specific settings
  device: "mps"  # Metal Performance Shaders for M1
  num_workers: 4
  pin_memory: false

  # M1-specific optimizations
  use_mps_fallback: true
  enable_metal_acceleration: true

notes:
  description: "Fast training configuration for quick iterations"
  expected_time_m1: "30-60 minutes"
  expected_time_gpu: "10-20 minutes (on T4/A10)"
  memory_usage: "~12GB on M1"
  use_case: "Rapid prototyping, testing pipeline, small domains"
